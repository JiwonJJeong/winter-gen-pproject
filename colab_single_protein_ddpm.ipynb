{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Protein DDPM Training\n",
    "\n",
    "This notebook trains a DDPM model on **one specific protein** by learning to denoise different conformational states (frames) from an MD trajectory.\n",
    "\n",
    "**Workflow:**\n",
    "1. Specify protein name and parameters\n",
    "2. Dynamically create/load trajectory data for that protein\n",
    "3. Train DDPM to denoise frames\n",
    "4. Generate and evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "PyTorch 2.5.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Check environment\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# Install dependencies (Colab only — locally assume deps are already installed)\n",
    "if IN_COLAB:\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', '-q', 'torch', 'torchvision', 'torchaudio'], check=True)\n",
    "    subprocess.run(['pip', 'install', '-q', 'omegaconf', 'pandas', 'tqdm', 'numpy', 'matplotlib', 'lightning', 'mdtraj', 'requests'], check=True)\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local mode; data dir: /home/jiwonjjeong/repos/winter-gen-pproject/data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Persist data on Drive so it survives session restarts\n",
    "    os.makedirs('/content/drive/MyDrive/protein_data/data', exist_ok=True)\n",
    "\n",
    "    if not os.path.exists('data'):\n",
    "        os.symlink('/content/drive/MyDrive/protein_data/data', 'data')\n",
    "    print(\"Drive mounted; data/ -> /content/drive/MyDrive/protein_data/data\")\n",
    "else:\n",
    "    # Running locally: just make sure data/ exists\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    print(f\"Local mode; data dir: {os.path.abspath('data')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get Code from GitHub\n",
    "\n",
    "Clone your repository to get the `gen_model` code (no data needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local repo root: /home/jiwonjjeong/repos/winter-gen-pproject\n",
      "README.md\n",
      "__init__.py\n",
      "__pycache__\n",
      "all_atom.py\n",
      "dataset.py\n",
      "diffusion\n",
      "ema.py\n",
      "geometry.py\n",
      "inference_se3.py.bak\n",
      "models\n",
      "parsing.py\n",
      "residue_constants.py\n",
      "rigid_utils.py\n",
      "simple_inference.py\n",
      "simple_train.py\n",
      "split.py\n",
      "splits\n",
      "tensor_utils.py\n",
      "test_dataset.py\n",
      "train_se3.py.bak\n",
      "utils.py\n",
      "\n",
      "Code ready\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/JiwonJJeong/winter-gen-pproject.git\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists('winter-gen-pproject'):\n",
    "        print(f\"Cloning {REPO_URL} ...\")\n",
    "        subprocess.run(['git', 'clone', REPO_URL], check=True)\n",
    "    os.chdir('winter-gen-pproject')\n",
    "    # Pull latest changes and clear bytecode cache\n",
    "    subprocess.run(['git', 'pull', 'origin', 'main'], check=True)\n",
    "    for cmd in [\n",
    "        'find gen_model -name \"*.pyc\" -delete',\n",
    "        'find gen_model -name \"__pycache__\" -type d -exec rm -rf {} + 2>/dev/null; true',\n",
    "    ]:\n",
    "        subprocess.run(cmd, shell=True)\n",
    "    print(\"Code updated\")\n",
    "else:\n",
    "    # Already inside the repo; just verify gen_model/ is present\n",
    "    if not os.path.isdir('gen_model'):\n",
    "        raise RuntimeError(\n",
    "            \"gen_model/ not found. Run this notebook from the repository root:\\n\"\n",
    "            \"  cd /path/to/winter-gen-pproject && jupyter notebook\"\n",
    "        )\n",
    "    print(f\"Local repo root: {os.path.abspath('.')}\")\n",
    "\n",
    "import subprocess as _sp\n",
    "result = _sp.run(['ls', 'gen_model/'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "print(\"Code ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Protein and Training\n",
    "\n",
    "**Customize these settings for your protein:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "================================================================================\n",
      "Protein: 4o66_C_R1\n",
      "Frames: 200\n",
      "SE(3) diffusion: rot=True, trans=True\n",
      "Score network: node_dim=256, edge_dim=128, IPA blocks=4\n",
      "Loss weights: rot=1.0, trans=1.0, psi=1.0\n",
      "Training epochs: 100\n",
      "Batch size: 8\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "protein_config = OmegaConf.create({\n",
    "\n",
    "    # ========== Protein Settings ==========\n",
    "    'protein': {\n",
    "        'name': '4o66_C',           # Protein name (without _R suffix)\n",
    "        'replica': 1,               # Replica number\n",
    "        'num_frames': 200,          # Number of trajectory frames to generate\n",
    "        'num_residues': 100,        # Number of residues in the protein\n",
    "\n",
    "        # Data split ratios (must sum to < 1.0)\n",
    "        'train_early_ratio': 0.3,   # Early training frames (30%)\n",
    "        'train_ratio': 0.4,         # Main training frames (40%)\n",
    "        'val_ratio': 0.2,           # Validation frames (20%)\n",
    "        # Remaining frames are test (~10%)\n",
    "    },\n",
    "\n",
    "    # ========== SE(3) Diffusion Settings ==========\n",
    "    'se3': {\n",
    "        'diffuse_rot': True,        # Diffuse rotations on SO(3)\n",
    "        'diffuse_trans': True,      # Diffuse translations on R^3\n",
    "        'so3': {\n",
    "            'schedule': 'logarithmic',\n",
    "            'min_sigma': 0.1,\n",
    "            'max_sigma': 1.5,\n",
    "            'num_sigma': 1000,\n",
    "            'use_cached_score': False,\n",
    "            'cache_dir': '/tmp/igso3_cache',\n",
    "            'num_omega': 1000,\n",
    "        },\n",
    "        'r3': {\n",
    "            'min_b': 0.1,\n",
    "            'max_b': 20.0,\n",
    "            'coordinate_scaling': 0.1,  # Angstrom → normalized units\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # ========== Score Network Architecture ==========\n",
    "    'score_model': {\n",
    "        'node_embed_size': 256,\n",
    "        'edge_embed_size': 128,\n",
    "        'embed': {\n",
    "            'index_embed_size': 32,\n",
    "            'embed_self_conditioning': True,\n",
    "            'num_bins': 22,\n",
    "            'min_bin': 1e-5,\n",
    "            'max_bin': 20.0,\n",
    "        },\n",
    "        'ipa': {\n",
    "            'c_s': 256,             # Must match node_embed_size\n",
    "            'c_z': 128,             # Must match edge_embed_size\n",
    "            'c_hidden': 16,\n",
    "            'no_heads': 12,\n",
    "            'no_qk_points': 4,\n",
    "            'no_v_points': 8,\n",
    "            'c_skip': 64,\n",
    "            'num_blocks': 4,\n",
    "            'coordinate_scaling': 0.1,\n",
    "            'seq_tfmr_num_heads': 4,\n",
    "            'seq_tfmr_num_layers': 2,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # ========== Training Settings ==========\n",
    "    'training': {\n",
    "        'batch_size': 8,            # Batch size (4-16 depending on GPU)\n",
    "        'num_epochs': 100,          # Number of training epochs (50-200)\n",
    "        'learning_rate': 1e-4,      # Learning rate (1e-5 to 5e-4)\n",
    "        'rot_loss_weight': 1.0,     # Weight for rotation score loss\n",
    "        'trans_loss_weight': 1.0,   # Weight for translation score loss\n",
    "        'psi_loss_weight': 1.0,     # Weight for psi torsion angle loss\n",
    "    },\n",
    "\n",
    "    # ========== Inference Settings ==========\n",
    "    'inference': {\n",
    "        'num_samples': 5,           # Number of samples to test\n",
    "        'num_steps': 200,           # Reverse diffusion steps\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"Configuration Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Protein: {protein_config.protein.name}_R{protein_config.protein.replica}\")\n",
    "print(f\"Frames: {protein_config.protein.num_frames}\")\n",
    "print(f\"SE(3) diffusion: rot={protein_config.se3.diffuse_rot}, trans={protein_config.se3.diffuse_trans}\")\n",
    "print(f\"Score network: node_dim={protein_config.score_model.node_embed_size}, \"\n",
    "      f\"edge_dim={protein_config.score_model.edge_embed_size}, \"\n",
    "      f\"IPA blocks={protein_config.score_model.ipa.num_blocks}\")\n",
    "print(f\"Loss weights: rot={protein_config.training.rot_loss_weight}, \"\n",
    "      f\"trans={protein_config.training.trans_loss_weight}, \"\n",
    "      f\"psi={protein_config.training.psi_loss_weight}\")\n",
    "print(f\"Training epochs: {protein_config.training.num_epochs}\")\n",
    "print(f\"Batch size: {protein_config.training.batch_size}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create/Load Protein Data\n",
    "\n",
    "This creates the data structure for your specified protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 4o66_C seems to exist at ./data/4o66_C. Skipping download.\n",
      "Detected timestep: 10.0 ps\n",
      "Preprocessing output ./data/4o66_C/4o66_C_R1_latent.npy already exists. Skipping.\n",
      "Preprocessing output ./data/4o66_C/4o66_C_R2_latent.npy already exists. Skipping.\n",
      "Preprocessing output ./data/4o66_C/4o66_C_R3_latent.npy already exists. Skipping.\n",
      "Creating 4-way split (Early: 5.0ns, Ratios: [0.6, 0.2, 0.2])...\n",
      "Found 3 trajectory file(s) for 4o66_C.\n",
      "  4o66_C_R1_latent: Total 10001, Timestep 10.0ps\n",
      "    Early: [0:500], Train: [500:6200], Val: [6200:8100], Test: [8100:10001]\n",
      "  4o66_C_R2_latent: Total 10001, Timestep 10.0ps\n",
      "    Early: [0:500], Train: [500:6200], Val: [6200:8100], Test: [8100:10001]\n",
      "  4o66_C_R3_latent: Total 10001, Timestep 10.0ps\n",
      "    Early: [0:500], Train: [500:6200], Val: [6200:8100], Test: [8100:10001]\n",
      "Updated splits saved to gen_model/splits/frame_splits.csv\n",
      "✅ Data ready at: data/4o66_C/4o66_C_R1_latent.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use the automatic download and prep script\n",
    "prot_name = protein_config.protein.name\n",
    "!python scripts/download_and_prep.py {prot_name} --data_dir ./data --out_dir ./data --suffix _latent\n",
    "\n",
    "# Setup paths for verification\n",
    "prot_cfg = protein_config.protein\n",
    "PROTEIN_FULL_NAME = f\"{prot_cfg.name}_R{prot_cfg.replica}\"\n",
    "protein_dir = f'data/{prot_cfg.name}'\n",
    "traj_path = f'{protein_dir}/{PROTEIN_FULL_NAME}_latent.npy'\n",
    "\n",
    "if os.path.exists(traj_path):\n",
    "    print(f\"✅ Data ready at: {traj_path}\")\n",
    "else:\n",
    "    print(f\"❌ Error: Data not found at {traj_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modules imported\n",
      "\n",
      "Dataset config:\n",
      "  Protein: 4o66_C\n",
      "  Replica: 1\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from gen_model.simple_train import SE3Module\n",
    "from gen_model.dataset import MDGenDataset\n",
    "from gen_model.diffusion.se3_diffuser import SE3Diffuser\n",
    "\n",
    "print(\"✓ Modules imported\")\n",
    "\n",
    "# Create dataset config\n",
    "data_config = OmegaConf.create({\n",
    "    'data_dir': 'data',\n",
    "    'atlas_csv': 'gen_model/splits/atlas.csv',\n",
    "    'train_split': 'gen_model/splits/frame_splits.csv',\n",
    "    'suffix': '_latent',\n",
    "    'frame_interval': None,\n",
    "    'crop_ratio': 0.95,\n",
    "    'min_t': 0.01,\n",
    "\n",
    "    # Single protein filters\n",
    "    'pep_name': prot_cfg.name,     # Only load this protein\n",
    "    'replica': prot_cfg.replica,   # Only load this replica\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset config:\")\n",
    "print(f\"  Protein: {data_config.pep_name}\")\n",
    "print(f\"  Replica: {data_config.replica}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising SE3Diffuser...\n",
      "✓ SE3Diffuser ready\n",
      "\n",
      "Loading datasets...\n",
      "\n",
      "Computing coordinate scale for 5700 frames...\n",
      "Dataset train mode: coord_scale = 0.1315\n",
      "Dataset val mode: coord_scale = 0.1000\n",
      "✓ Training frames: 5700\n",
      "✓ Validation frames: 1900\n",
      "\n",
      "Sample keys: ['aatype', 'seq_idx', 'chain_idx', 'res_mask', 'fixed_mask', 'rigids_0', 'atom37_pos', 'atom14_pos', 'torsion_angles_sin_cos', 'rigids_t', 'trans_score', 'rot_score', 'trans_score_scaling', 'rot_score_scaling', 't', 'sc_ca_t', 'residx_atom14_to_atom37', 'atom37_mask', 'residue_index', 'name', 'frame_indices', 'centroid', 'coord_scale']\n",
      "Residues: 76\n",
      "rigids_t shape: torch.Size([76, 7])  (per-residue SE3 frame at time t)\n",
      "rot_score shape: (76, 3)\n",
      "trans_score shape: (76, 3)\n",
      "t (noise level): 0.6642\n"
     ]
    }
   ],
   "source": [
    "# Create SE3Diffuser (initialises IGSO3 look-up tables — takes ~10 s first run)\n",
    "print(\"Initialising SE3Diffuser...\")\n",
    "se3_diffuser = SE3Diffuser(protein_config.se3)\n",
    "print(\"✓ SE3Diffuser ready\\n\")\n",
    "\n",
    "# Create datasets with diffuser so each batch already contains\n",
    "# rigids_t, rot_score, trans_score computed from the forward process\n",
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "train_dataset = MDGenDataset(\n",
    "    args=data_config,\n",
    "    diffuser=se3_diffuser,\n",
    "    mode='train',\n",
    "    repeat=1,\n",
    "    num_consecutive=1,\n",
    "    stride=1\n",
    ")\n",
    "\n",
    "val_dataset = MDGenDataset(\n",
    "    args=data_config,\n",
    "    diffuser=se3_diffuser,\n",
    "    mode='val',\n",
    "    repeat=1,\n",
    "    num_consecutive=1,\n",
    "    stride=1\n",
    ")\n",
    "val_dataset.coord_scale = float(train_dataset.coord_scale)\n",
    "\n",
    "print(f\"✓ Training frames: {len(train_dataset)}\")\n",
    "print(f\"✓ Validation frames: {len(val_dataset)}\")\n",
    "\n",
    "# Inspect a sample to confirm keys\n",
    "sample = train_dataset[0]\n",
    "n_residues = sample['res_mask'].shape[0]\n",
    "print(f\"\\nSample keys: {list(sample.keys())}\")\n",
    "print(f\"Residues: {n_residues}\")\n",
    "print(f\"rigids_t shape: {sample['rigids_t'].shape}  (per-residue SE3 frame at time t)\")\n",
    "print(f\"rot_score shape: {sample['rot_score'].shape}\")\n",
    "print(f\"trans_score shape: {sample['trans_score'].shape}\")\n",
    "print(f\"t (noise level): {sample['t']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'residue_constants' from 'data' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# SE3Module wraps ScoreNetwork (IPA-based) + SE3Diffuser (for score computation)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model_pl \u001b[38;5;241m=\u001b[39m \u001b[43mSE3Module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_conf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotein_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mse3_conf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotein_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mse3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotein_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrot_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotein_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrot_loss_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrans_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotein_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrans_loss_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpsi_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotein_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpsi_loss_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m n_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model_pl\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ SE3 Score Network (IPA): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parameters (~\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_params\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/winter-gen-pproject/./gen_model/simple_train.py:42\u001b[0m, in \u001b[0;36mSE3Module.__init__\u001b[0;34m(self, model_conf, se3_conf, lr, rot_loss_weight, trans_loss_weight, psi_loss_weight)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters(ignore\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_conf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mse3_conf\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgen_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mse3_diffuser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SE3Diffuser\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgen_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscore_network\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ScoreNetwork\n\u001b[1;32m     43\u001b[0m diffuser \u001b[38;5;241m=\u001b[39m SE3Diffuser(se3_conf)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m ScoreNetwork(model_conf, diffuser)\n",
      "File \u001b[0;32m~/repos/winter-gen-pproject/./gen_model/models/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Models module for SE(3) diffusion score networks.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgen_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscore_network\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ScoreNetwork\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgen_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mipa_pytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IpaScore, InvariantPointAttention\n\u001b[1;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScoreNetwork\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIpaScore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvariantPointAttention\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/repos/winter-gen-pproject/./gen_model/models/score_network.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgen_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils \u001b[38;5;28;01mas\u001b[39;00m du\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgen_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m all_atom\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgen_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ipa_pytorch\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfn\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/winter-gen-pproject/./gen_model/all_atom.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"Utilities for calculating all atom representations.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m residue_constants\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenfold\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rigid_utils \u001b[38;5;28;01mas\u001b[39;00m ru\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenfold\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data_transforms\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'residue_constants' from 'data' (unknown location)"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# SE3Module wraps ScoreNetwork (IPA-based) + SE3Diffuser (for score computation)\n",
    "model_pl = SE3Module(\n",
    "    model_conf=protein_config.score_model,\n",
    "    se3_conf=protein_config.se3,\n",
    "    lr=protein_config.training.learning_rate,\n",
    "    rot_loss_weight=protein_config.training.rot_loss_weight,\n",
    "    trans_loss_weight=protein_config.training.trans_loss_weight,\n",
    "    psi_loss_weight=protein_config.training.psi_loss_weight,\n",
    ")\n",
    "\n",
    "n_params = sum(p.numel() for p in model_pl.parameters())\n",
    "print(f\"✓ SE3 Score Network (IPA): {n_params:,} parameters (~{n_params*4/1024/1024:.1f} MB)\")\n",
    "print(f\"  node_embed_size: {protein_config.score_model.node_embed_size}\")\n",
    "print(f\"  edge_embed_size: {protein_config.score_model.edge_embed_size}\")\n",
    "print(f\"  IPA blocks: {protein_config.score_model.ipa.num_blocks}\")\n",
    "print(f\"  Loss: rot + trans (score-matching) + psi (sin/cos MSE)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=protein_config.training.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=protein_config.training.batch_size, shuffle=False)\n",
    "\n",
    "# 2. Checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f'checkpoints/{prot_cfg.name}_se3',\n",
    "    filename='se3-{epoch:02d}-{val_loss:.4f}',\n",
    "    save_top_k=3,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "# 3. Train  (model_pl created in Step 7)\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=protein_config.training.num_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    precision=\"16-mixed\" if torch.cuda.is_available() else 32,\n",
    ")\n",
    "\n",
    "trainer.fit(model_pl, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "import glob\n",
    "ckpts = sorted(glob.glob(f\"checkpoints/{prot_cfg.name}_ddpm/*.ckpt\"))\n",
    "if ckpts:\n",
    "    model, epoch, loss, coord_scale = load_checkpoint(ckpts[-1], model, device)\n",
    "    print(f\"✓ Loaded checkpoint from epoch {epoch}\")\n",
    "\n",
    "# Test with unscaling\n",
    "results = test_with_dataset(\n",
    "    model, diffusion, val_dataset, device,\n",
    "    coord_scale=coord_scale,\n",
    "    num_samples=protein_config.inference.num_samples\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new conformations from noise\n",
    "print(\"Generating new samples from noise...\\n\")\n",
    "\n",
    "shape = (3, n_residues, in_channels)\n",
    "generated = sample_from_noise(\n",
    "    model, diffusion, shape, device,\n",
    "    num_steps=protein_config.inference.denoise_steps\n",
    ")\n",
    "\n",
    "# Save\n",
    "output_dir = f'outputs/{prot_cfg.name}_ddpm'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(3):\n",
    "    path = f'{output_dir}/generated_sample_{i}.npy'\n",
    "    np.save(path, generated[i].cpu().numpy())\n",
    "    print(f\"  Saved: {path}\")\n",
    "\n",
    "# Save test results\n",
    "for i, r in enumerate(results):\n",
    "    np.save(f'{output_dir}/test_{i}_original.npy', r['original'])\n",
    "    np.save(f'{output_dir}/test_{i}_denoised.npy', r['denoised'])\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mse_values = [r['mse'] for r in results]\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(range(len(mse_values)), mse_values, color='steelblue')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('MSE')\n",
    "plt.title(f'Reconstruction Error\\n{PROTEIN_FULL_NAME}')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(mse_values, bins=10, edgecolor='black', color='steelblue')\n",
    "plt.xlabel('MSE')\n",
    "plt.ylabel('Count')\n",
    "plt.title('MSE Distribution')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(mse_values, 'o-', color='steelblue')\n",
    "plt.axhline(np.mean(mse_values), color='red', linestyle='--', label='Mean')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE Trend')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: {output_dir}/analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package results\n",
    "!zip -rq {prot_cfg.name}_results.zip {save_dir} {output_dir}\n",
    "\n",
    "print(f\"\\nResults packaged: {prot_cfg.name}_results.zip\")\n",
    "print(f\"  Checkpoints: {save_dir}/\")\n",
    "print(f\"  Outputs: {output_dir}/\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(f'{prot_cfg.name}_results.zip')\n",
    "    print(\"\\n✓ Download started\")\n",
    "else:\n",
    "    print(f\"\\n✓ Saved locally as {prot_cfg.name}_results.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we did:**\n",
    "1. ✓ Configured protein: `{protein_config.protein.name}_R{protein_config.protein.replica}`\n",
    "2. ✓ Downloaded and preprocessed trajectory data using \n",
    "download_and_prep.py\n",
    "3. ✓ Trained DDPM for {protein_config.training.num_epochs} epochs\n",
    "4. ✓ Evaluated denoising on validation frames\n",
    "5. ✓ Generated new conformations from noise\n",
    "\n",
    "**Key insight:** The model learned the conformational space of **one specific protein** by training on different frames from its MD trajectory.\n",
    "\n",
    "**To train on a different protein:**\n",
    "- Change `protein_config.protein.name`\n",
    "- Rerun from Step 3 onwards"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
