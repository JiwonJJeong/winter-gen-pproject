{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Simple DDPM Training and Inference for Protein Frames\n",
    "\n",
    "This notebook trains a basic DDPM model to denoise protein frames from MD trajectories.\n",
    "\n",
    "**What this does:**\n",
    "- Loads single frames from MD trajectory data\n",
    "- Adds noise according to a diffusion schedule\n",
    "- Trains a model to predict and remove the noise\n",
    "- Performs inference to denoise frames\n",
    "\n",
    "**Steps:**\n",
    "1. Setup environment and upload data\n",
    "2. Configure hyperparameters\n",
    "3. Train the model\n",
    "4. Run inference and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running on Google Colab\")\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q omegaconf pandas tqdm numpy\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload"
   },
   "source": [
    "## 2. Upload Project Files\n",
    "\n",
    "You have two options:\n",
    "1. **Option A**: Upload the entire `gen_model` directory as a zip file\n",
    "2. **Option B**: Clone from GitHub (if you have a repo)\n",
    "\n",
    "Choose one of the options below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_option_a"
   },
   "outputs": [],
   "source": [
    "# Option A: Upload gen_model.zip\n",
    "# 1. On your local machine: zip -r gen_model.zip gen_model/ data/\n",
    "# 2. Upload the zip file using the code below\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"Upload gen_model.zip (containing gen_model/ and data/ directories)\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Extract\n",
    "    !unzip -q gen_model.zip\n",
    "    print(\"✓ Files extracted\")\n",
    "else:\n",
    "    print(\"Not on Colab - skip this step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_option_b"
   },
   "outputs": [],
   "source": [
    "# Option B: Clone from GitHub\n",
    "# Uncomment and modify the line below if you have a GitHub repo\n",
    "\n",
    "# !git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git\n",
    "# %cd YOUR_REPO\n",
    "# print(\"✓ Repository cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_files"
   },
   "outputs": [],
   "source": [
    "# Verify files are present\n",
    "import os\n",
    "print(\"Checking for required files...\")\n",
    "print(f\"gen_model exists: {os.path.exists('gen_model')}\")\n",
    "print(f\"data exists: {os.path.exists('data')}\")\n",
    "print(f\"simple_train.py exists: {os.path.exists('gen_model/simple_train.py')}\")\n",
    "print(f\"simple_inference.py exists: {os.path.exists('gen_model/simple_inference.py')}\")\n",
    "print(f\"dataset.py exists: {os.path.exists('gen_model/dataset.py')}\")\n",
    "\n",
    "# List data files\n",
    "if os.path.exists('data'):\n",
    "    !ls -lh data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyperparameters"
   },
   "source": [
    "## 3. Configure Hyperparameters\n",
    "\n",
    "**Key hyperparameters to tune:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAMETERS - Adjust these for your experiments\n",
    "# ============================================================================\n",
    "\n",
    "config = OmegaConf.create({\n",
    "    # Dataset configuration\n",
    "    'data': {\n",
    "        'data_dir': 'data',\n",
    "        'atlas_csv': 'data/atlas.csv',\n",
    "        'train_split': 'gen_model/splits/frame_splits.csv',\n",
    "        'suffix': '_latent',  # File suffix for .npy files\n",
    "        'frame_interval': None,  # Sample every N frames (None = all frames)\n",
    "        'crop_ratio': 0.95,  # Ratio of residues to keep (0.95 = 95%)\n",
    "        'min_t': 0.01,  # Minimum diffusion timestep\n",
    "    },\n",
    "    \n",
    "    # Diffusion configuration\n",
    "    'diffusion': {\n",
    "        'timesteps': 1000,  # Number of diffusion steps (100-1000 typical)\n",
    "        'beta_start': 0.0001,  # Starting noise level (smaller = less noise initially)\n",
    "        'beta_end': 0.02,  # Ending noise level (larger = more noise at end)\n",
    "        # Note: Can try cosine schedule for potentially better results\n",
    "    },\n",
    "    \n",
    "    # Model architecture\n",
    "    'model': {\n",
    "        'hidden_dim': 256,  # Hidden layer size (128-512 typical)\n",
    "        'time_emb_dim': 128,  # Time embedding dimension (64-256 typical)\n",
    "        # Larger = more capacity but slower training\n",
    "    },\n",
    "    \n",
    "    # Training configuration\n",
    "    'training': {\n",
    "        'batch_size': 8,  # Batch size (reduce if OOM, increase for faster training)\n",
    "        'num_epochs': 100,  # Number of training epochs (50-200 typical)\n",
    "        'learning_rate': 1e-4,  # Learning rate (1e-5 to 1e-3 typical)\n",
    "        'num_workers': 2,  # DataLoader workers (0-4 on Colab)\n",
    "        'save_every': 10,  # Save checkpoint every N epochs\n",
    "    },\n",
    "    \n",
    "    # Checkpoint configuration\n",
    "    'checkpoint': {\n",
    "        'save_dir': 'checkpoints/simple_ddpm',\n",
    "        'load_from': None,  # Path to checkpoint to resume from (or None)\n",
    "    },\n",
    "    \n",
    "    # Inference configuration\n",
    "    'inference': {\n",
    "        'num_samples': 5,  # Number of samples to test\n",
    "        'output_dir': 'outputs/simple_ddpm',\n",
    "        'denoise_steps': 1000,  # Number of denoising steps (can be < training steps)\n",
    "    },\n",
    "})\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyperparameter_notes"
   },
   "source": [
    "### Hyperparameter Tuning Guide\n",
    "\n",
    "| Hyperparameter | Description | Typical Range | Impact |\n",
    "|----------------|-------------|---------------|--------|\n",
    "| **timesteps** | Number of diffusion steps | 100-1000 | More steps = better quality but slower |\n",
    "| **beta_start** | Initial noise level | 0.0001-0.001 | Lower = gradual noising |\n",
    "| **beta_end** | Final noise level | 0.01-0.05 | Higher = more noise |\n",
    "| **hidden_dim** | Model capacity | 128-512 | Larger = more capacity, slower |\n",
    "| **batch_size** | Samples per iteration | 4-32 | Larger = faster but needs more memory |\n",
    "| **num_epochs** | Training iterations | 50-200 | More = better but can overfit |\n",
    "| **learning_rate** | Optimization step size | 1e-5 to 1e-3 | Too high = unstable, too low = slow |\n",
    "\n",
    "**Recommended starting points:**\n",
    "- **Quick test**: timesteps=100, hidden_dim=128, batch_size=16, num_epochs=50\n",
    "- **Balanced**: timesteps=500, hidden_dim=256, batch_size=8, num_epochs=100 (default)\n",
    "- **High quality**: timesteps=1000, hidden_dim=512, batch_size=4, num_epochs=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 4. Import Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_modules"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add gen_model to path\n",
    "if 'gen_model' not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath('.'))\n",
    "\n",
    "# Import training modules\n",
    "from gen_model.simple_train import SimpleDDPM, SimpleDenoiseModel, train_ddpm\n",
    "from gen_model.simple_inference import (\n",
    "    denoise_step, \n",
    "    sample_from_noise, \n",
    "    denoise_frame,\n",
    "    load_checkpoint,\n",
    "    test_with_dataset\n",
    ")\n",
    "from gen_model.dataset import MDGenDataset\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"✓ Modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset"
   },
   "source": [
    "## 5. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset"
   },
   "outputs": [],
   "source": [
    "# Create training dataset\n",
    "print(\"Loading training dataset...\")\n",
    "train_dataset = MDGenDataset(\n",
    "    args=config.data,\n",
    "    diffuser=None,  # No SE3 diffuser needed for basic DDPM\n",
    "    split=config.data.train_split,\n",
    "    mode='train',\n",
    "    repeat=1,\n",
    "    num_consecutive=1,\n",
    "    stride=1\n",
    ")\n",
    "\n",
    "print(f\"✓ Training dataset size: {len(train_dataset)}\")\n",
    "\n",
    "# Create validation dataset\n",
    "print(\"Loading validation dataset...\")\n",
    "val_dataset = MDGenDataset(\n",
    "    args=config.data,\n",
    "    diffuser=None,\n",
    "    split=config.data.train_split,\n",
    "    mode='val',\n",
    "    repeat=1,\n",
    "    num_consecutive=1,\n",
    "    stride=1\n",
    ")\n",
    "\n",
    "print(f\"✓ Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Get sample to determine input dimensions\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample keys: {sample.keys()}\")\n",
    "\n",
    "if 'atom14_pos' in sample:\n",
    "    sample_data = sample['atom14_pos']\n",
    "    data_key = 'atom14_pos'\n",
    "elif 'rigids_0' in sample:\n",
    "    sample_data = sample['rigids_0'][..., 4:]\n",
    "    data_key = 'rigids_0'\n",
    "else:\n",
    "    raise ValueError(\"Sample must contain 'atom14_pos' or 'rigids_0'\")\n",
    "\n",
    "print(f\"Using data key: {data_key}\")\n",
    "print(f\"Sample data shape: {sample_data.shape}\")\n",
    "\n",
    "# Determine input dimensions\n",
    "if len(sample_data.shape) == 3:\n",
    "    # [N_res, N_atoms, 3]\n",
    "    n_residues = sample_data.shape[0]\n",
    "    in_channels = sample_data.shape[1] * sample_data.shape[2]\n",
    "else:\n",
    "    # [N_res, 3] or [N_res, C]\n",
    "    n_residues = sample_data.shape[0]\n",
    "    in_channels = sample_data.shape[1]\n",
    "\n",
    "print(f\"Number of residues: {n_residues}\")\n",
    "print(f\"Input channels (flattened): {in_channels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_init"
   },
   "source": [
    "## 6. Initialize Model and Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_model"
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create diffusion scheduler\n",
    "diffusion = SimpleDDPM(\n",
    "    timesteps=config.diffusion.timesteps,\n",
    "    beta_start=config.diffusion.beta_start,\n",
    "    beta_end=config.diffusion.beta_end\n",
    ")\n",
    "diffusion = diffusion.to(device)\n",
    "print(f\"✓ Diffusion scheduler created with {config.diffusion.timesteps} timesteps\")\n",
    "\n",
    "# Create model\n",
    "model = SimpleDenoiseModel(\n",
    "    in_channels=in_channels,\n",
    "    hidden_dim=config.model.hidden_dim,\n",
    "    time_emb_dim=config.model.time_emb_dim\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"✓ Model created\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_ddpm(\n",
    "    dataset=train_dataset,\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    device=device,\n",
    "    batch_size=config.training.batch_size,\n",
    "    num_epochs=config.training.num_epochs,\n",
    "    lr=config.training.learning_rate,\n",
    "    save_dir=config.checkpoint.save_dir,\n",
    "    save_every=config.training.save_every\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## 8. Inference and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_checkpoint"
   },
   "outputs": [],
   "source": [
    "# Load the best checkpoint\n",
    "import glob\n",
    "\n",
    "checkpoint_files = sorted(glob.glob(f\"{config.checkpoint.save_dir}/*.pt\"))\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = checkpoint_files[-1]\n",
    "    print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    model, epoch, loss = load_checkpoint(latest_checkpoint, model, device)\n",
    "    model = model.to(device)\n",
    "    print(f\"✓ Loaded checkpoint from epoch {epoch}\")\n",
    "else:\n",
    "    print(\"No checkpoints found. Using current model state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_denoising"
   },
   "outputs": [],
   "source": [
    "# Test denoising on validation set\n",
    "print(\"\\nTesting denoising on validation samples...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = test_with_dataset(\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    dataset=val_dataset,\n",
    "    device=device,\n",
    "    num_samples=config.inference.num_samples\n",
    ")\n",
    "\n",
    "# Calculate statistics\n",
    "mse_values = [r['mse'] for r in results]\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Results Summary:\")\n",
    "print(f\"  Average MSE: {np.mean(mse_values):.6f}\")\n",
    "print(f\"  Std MSE: {np.std(mse_values):.6f}\")\n",
    "print(f\"  Min MSE: {np.min(mse_values):.6f}\")\n",
    "print(f\"  Max MSE: {np.max(mse_values):.6f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "import os\n",
    "os.makedirs(config.inference.output_dir, exist_ok=True)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    np.save(f\"{config.inference.output_dir}/test_{i}_original.npy\", result['original'])\n",
    "    np.save(f\"{config.inference.output_dir}/test_{i}_noisy.npy\", result['noisy'])\n",
    "    np.save(f\"{config.inference.output_dir}/test_{i}_denoised.npy\", result['denoised'])\n",
    "\n",
    "print(f\"✓ Results saved to {config.inference.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize"
   },
   "outputs": [],
   "source": [
    "# Visualize reconstruction quality\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot MSE distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(mse_values)), mse_values)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Reconstruction Error per Sample')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(mse_values, bins=10, edgecolor='black')\n",
    "plt.xlabel('MSE')\n",
    "plt.ylabel('Count')\n",
    "plt.title('MSE Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config.inference.output_dir}/mse_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Visualization saved to {config.inference.output_dir}/mse_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sample_generation"
   },
   "source": [
    "## 9. Generate New Samples from Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_samples"
   },
   "outputs": [],
   "source": [
    "# Generate samples from pure noise\n",
    "print(\"Generating samples from random noise...\")\n",
    "\n",
    "num_samples_to_generate = 3\n",
    "shape = (num_samples_to_generate, n_residues, in_channels)\n",
    "\n",
    "generated_samples = sample_from_noise(\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    shape=shape,\n",
    "    device=device,\n",
    "    num_steps=config.inference.denoise_steps\n",
    ")\n",
    "\n",
    "# Save generated samples\n",
    "for i in range(num_samples_to_generate):\n",
    "    sample_path = f\"{config.inference.output_dir}/generated_sample_{i}.npy\"\n",
    "    np.save(sample_path, generated_samples[i].cpu().numpy())\n",
    "    print(f\"  Saved: {sample_path}\")\n",
    "\n",
    "print(f\"\\n✓ Generated {num_samples_to_generate} samples from noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 10. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_results"
   },
   "outputs": [],
   "source": [
    "# Create zip file with results\n",
    "!zip -r results.zip {config.checkpoint.save_dir} {config.inference.output_dir}\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download('results.zip')\n",
    "    print(\"✓ Results downloaded\")\n",
    "else:\n",
    "    print(\"Results saved locally in results.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary\n",
    "\n",
    "**What we did:**\n",
    "1. ✓ Set up environment and uploaded data\n",
    "2. ✓ Configured hyperparameters\n",
    "3. ✓ Loaded MD trajectory dataset\n",
    "4. ✓ Created DDPM model and diffusion scheduler\n",
    "5. ✓ Trained the model\n",
    "6. ✓ Tested denoising on validation set\n",
    "7. ✓ Generated new samples from noise\n",
    "8. ✓ Saved and downloaded results\n",
    "\n",
    "**Next steps to improve:**\n",
    "- Tune hyperparameters (see tuning guide above)\n",
    "- Try longer training (more epochs)\n",
    "- Experiment with different model architectures\n",
    "- Add SE3 equivariance for better protein structure modeling\n",
    "- Implement conditional generation (condition on sequence, etc.)\n",
    "\n",
    "**Files saved:**\n",
    "- Checkpoints: `{config.checkpoint.save_dir}/`\n",
    "- Results: `{config.inference.output_dir}/`\n",
    "- Visualizations: `{config.inference.output_dir}/mse_analysis.png`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
